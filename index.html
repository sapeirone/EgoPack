<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Human understanding of a video stream is naturally broad. To transfer this perception to intelligent machines, learning abstracting knowledge from different perspectives plays a crucial role. EgoPack creates a backpack of skills that an agent can reuse when it needs to learn a novel task.">
  <meta property="og:title" content="A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives"/>
  <meta property="og:description" content="Human understanding of a video stream is naturally broad. To transfer this perception to intelligent machines, learning abstracting knowledge from different perspectives plays a crucial role. EgoPack creates a backpack of skills that an agent can reuse when it needs to learn a novel task."/>
  <meta property="og:url" content="sapeirone.github.io/EgoPack"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/teaser.png" />
  <!--meta property="og:image:width" content="1200"/-->
  <!--meta property="og:image:height" content="630"/-->


  <meta name="twitter:title" content="A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives">
  <meta name="twitter:description" content="Human understanding of a video stream is naturally broad. To transfer this perception to intelligent machines, learning abstracting knowledge from different perspectives plays a crucial role. EgoPack creates a backpack of skills that an agent can reuse when it needs to learn a novel task.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/image/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Egocentric Vision; Computer Vision; Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="static/js/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style type="text/css">
    
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Backpack Full of Skills:<br>Egocentric Video Understanding with Diverse Task Perspectives</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="https://scholar.google.com/citations?user=K0efPssAAAAJ" target="_blank">Simone Alberto Peirone</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=7MJdvzYAAAAJ" target="_blank">Francesca Pistilli</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=yQqW5q0AAAAJ" target="_blank">Antonio Alliegro</a><sup>1,2</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=i4rm0tYAAAAJ" target="_blank">Giuseppe Averta</a><sup>1</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>Politecnico di Torino, &nbsp;&nbsp;<sup>2</sup>Istituto Italiano di Tecnologia
                      <br>
                      <small><tt>simone.peirone@polito.it</tt></small>
                      <br>
                      <b>Computer Vision and Pattern Recognition (CVPR) 2024</b>
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Peirone_A_Backpack_Full_of_Skills_Egocentric_Video_Understanding_with_Diverse_CVPR_2024_paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://www.youtube.com/watch?v=roKNbBP5AXY" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-video"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/sapeirone/EgoPack" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.03037" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop" style="width: calc(100% - 32px); max-width: 720px; margin-bottom: 32px;">
    <iframe style="width: 100% !important; aspect-ratio: 16/9;"
    src="https://www.youtube.com/embed/roKNbBP5AXY?vq=hd720" 
    title="A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives" frameborder="0" 
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed. We demonstrate the effectiveness and efficiency of our approach on four Ego4d benchmarks, outperforming current state-of-the-art methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">What can we learn from a video?</h3>
      <p>
        Different egocentric video tasks can provide <b>different, and possibly complementary, perspectives</b> on what the user is doing. 
        For example, learning to recognise human actions can give clues as to which objects are being manipulated or what will happen next.
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/what_can_we_learn.png" alt="" style="max-width: 768px; margin: auto;" />
      </figure>
      <h4 class="title is-4">How can we learn from these perspectives?</h4>
      <p>
        Different approaches can be used to learn from these tasks. 
        <b>Single task</b> models learn unique weights for each task. 
        <b>Multi-task learning</b> shares a common backbone among the different tasks, with small task-specific heads on top. 
        <b>Task translation</b> is a more recent approach that learns to translate the contributions of different task to solve one of them.
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/related.png" alt="" style="max-width: 768px; margin: auto;" />
      </figure>
      <p>
        All these approaches have some limitations. 
        For example, <b>multi-task learning</b> can share weights across different tasks, but it <b>does not explicitly model cross-task sinergies</b> and can lead to negative transfer between tasks.
        <br>
        Likewise, the <b>cross-task translation</b> mechanism proposed by EgoT2 combines perspectives from different tasks but it <b>needs to know all tasks before-hand</b> and requires separate models for each task.
      </p>
      <br>
      <h4 class="title is-4">A new paradigm for Egocentric Video Understanding</h4>
      <p>
        We propose an approach for <b>egocentric video understanding</b> that focuses on <b>knowledge reuse</b> across different tasks. 
        To do so, we adopt a graph-based shared model and the goal is to outperform single and multi-task baselines adapted to our scenario.
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/paradigm.png" alt="" style="max-width: 768px; margin: auto;" />
      </figure>
    </div>
  </div>
</section>

<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">Proposed Architecture</h3>
      <p>
        Our approach is called <b>EgoPack</b> and is composed of two stages. 
        In the first stage, a multi-task model is trained on a set of K known tasks. 
        In the second stage, the model is fine-tuned on a novel task using egopack’s cross-task interaction mechanism
      </p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/method.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
      </figure>

      <h4 class="title is-4">Step 1: MTL Pre-training step</h4>
      <div class="columns is-vcentered is-flex-center is-multiline">
        <div class="column is-half">
          <div class="text-block">
            <p>
              To share the same model across different tasks, we propose to <b>model video as graphs</b> whose nodes correspond to temporal segments of the video, 
              edges connect temporally close nodes and egocentric video tasks can be represented as different graph operations
            </p>
          </div>
        </div>
        <div class="column is-half">
          <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
            <img src="static/images/temp.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
          </figure>
        </div>
        <div class="column is-half">
          <div class="text-block">
            <p>
              With this architecture, we can model <b>temporal reasoning</b> using a graph neural network 
              that iteratively updates the nodes representations using <b>message passing</b>.
              <br><br>
              Finally, a set of <b>task-specific heads</b> project the nodes in the output space of each task.
            </p>
          </div>
        </div>
          <div class="column is-half">
            <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
              <img src="static/images/mtl.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
            </figure>
          </div>
      </div>


      <h4 class="title is-4">Step 2: Novel Task Learning</h2>
        <div class="columns is-vcentered is-flex-center is-multiline">
          <div class="column is-half">
            <div class="text-block">
              <p>
                These heads model different and complementary perspectives on the content of the video. 
                We can collect these perspectives in a set of action-wise <b>task-specific prototypes</b>.
                <br><br>
                We call these prototypes a <b>backpack of skills</b> and they represent a frozen snapshot of what the 
                model has learnt in the pre-training phase.
              </p>
            </div>
          </div>
          <div class="column is-half">
            <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
              <img src="static/images/egopack.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
            </figure>
          </div>
          <div class="column is-half">
            <div class="text-block">
              <p>
                <ol>
                  <li>
                    When learning a novel task, we feed the temporal features through the 
                    task-specific heads of the pre-training tasks.
                  </li>
                  <br>
                  <li>
                    These features act as queries to look for the closest matching prototypes 
                    using k-NN in the features space.
                  </li>
                  <br>
                  <li>
                    We refine the task features using <b>Message Passing with task prototypes</b>.
                  </li>
                </ol>
                
              </p>
            </div>
          </div>
            <div class="column is-half">
              <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
                <img src="static/images/fusion.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
              </figure>
            </div>
        </div>

    </div>
  </div>
</section>

<section class="section hero" style="padding-bottom: .75rem; padding-top: 1.5rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-bottom: .75rem; padding-top: 1.5rem;">
      <h3 class="title is-3">Experimental results</h3>
      <p>We validate EgoPack on Action Recognition (AR), Object State Change Classification (OSCC), Point of No Return (PNR) and Long Term Anticipation (LTA) from Ego4D.</p>
      <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
        <img src="static/images/results_val.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
      </figure>
      <div class="columns is-vcentered is-flex-center is-multiline">
        <div class="column is-half">
          <div class="text-block">
            <h5 class="title is-5">Cross-tasks agreement ratio</h5>
            <p>
              How much different <i>perspectives</i> bring complementary information?
            </p>
          </div>
        </div>
        <div class="column is-half">
          <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
            <img src="static/images/viz_agreement.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
          </figure>
        </div>
        <div class="column is-half">
          <figure class="image" style="margin-top: 32px; margin-bottom: 32px;">
            <img src="static/images/queries.png" alt="" style="max-width: 768px; margin: auto; margin-bottom: 16px;" />
          </figure>
        </div>
        <div class="column is-half">
          <div class="text-block">
            <h5 class="title is-5" style="text-align: right;">Queried prototypes</h5>
            <p style="text-align: right;">
              When the novel task is OSCC, what are the closest prototypes from the AR and PNR tasks?
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Cite us</h2>
      <pre><code>
@InProceedings{peirone2024backpack,
  author    = {Peirone, Simone Alberto and Pistilli, Francesca and Alliegro, Antonio and Averta, Giuseppe},
  title     = {A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2024},
  pages     = {18275-18285}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
