<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Human understanding of a video stream is naturally broad. To transfer this perception to intelligent machines, learning abstracting knowledge from different perspectives plays a crucial role. EgoPack creates a backpack of skills that an agent can reuse when it needs to learn a novel task.">
  <meta property="og:title" content="A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives"/>
  <meta property="og:description" content="Human understanding of a video stream is naturally broad. To transfer this perception to intelligent machines, learning abstracting knowledge from different perspectives plays a crucial role. EgoPack creates a backpack of skills that an agent can reuse when it needs to learn a novel task."/>
  <meta property="og:url" content="sapeirone.github.io/EgoPack"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/teaser.png" />
  <!--meta property="og:image:width" content="1200"/-->
  <!--meta property="og:image:height" content="630"/-->


  <meta name="twitter:title" content="A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives">
  <meta name="twitter:description" content="Human understanding of a video stream is naturally broad. To transfer this perception to intelligent machines, learning abstracting knowledge from different perspectives plays a crucial role. EgoPack creates a backpack of skills that an agent can reuse when it needs to learn a novel task.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/image/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Egocentric Vision; Computer Vision; Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="static/js/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Backpack Full of Skills:<br>Egocentric Video Understanding with Diverse Task Perspectives</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="https://scholar.google.com/citations?user=K0efPssAAAAJ" target="_blank">Simone Alberto Peirone</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=7MJdvzYAAAAJ" target="_blank">Francesca Pistilli</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=yQqW5q0AAAAJ" target="_blank">Antonio Alliegro</a><sup>1,2</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=i4rm0tYAAAAJ" target="_blank">Giuseppe Averta</a><sup>1</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>Politecnico di Torino, &nbsp;&nbsp;<sup>2</sup>Istituto Italiano di Tecnologia
                      <br>
                      <small><tt>firstname.lastname@polito.it</tt></small>
                      <br>
                      Computer Vision and Pattern Recognition (CVPR) 2024
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#" target="_blank"
                        class="external-link button is-normal is-rounded is-dark" disabled>
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/sapeirone/EgoPack" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark" disabled>
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!--video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video -->
      <figure class="image">
        <img src="static/images/teaser.png" alt="" style="max-width: 384px; margin: auto; margin-bottom: 32px;" />
      </figure>
      <h2 class="subtitle has-text-centered">
        Given a video stream, a robot is asked to learn a novel task, e.g. Object State Change Classification (OSCC). 
        To learn the new skill, the robot can access previously gained knowledge regarding different tasks, such Point of No Return (PNR), Long Term Anticipation (LTA) and Action Recognition (AR), and use it during the learning process to enhance downstream task performance. 
        This knowledge is stored as graphs inside the robot's backpack, always ready to boost a new skill.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Human comprehension of a video stream is naturally broad: in a few instants, we are able to understand what is happening, the relevance and relationship of objects, and forecast what will follow in the near future, everything all at once. We believe that - to effectively transfer such an holistic perception to intelligent machines - an important role is played by learning to correlate concepts and to abstract knowledge coming from different tasks, to synergistically exploit them when learning novel skills. To accomplish this, we seek for a unified approach to video understanding which combines shared temporal modelling of human actions with minimal overhead, to support multiple downstream tasks and enable cooperation when learning novel skills. We then propose EgoPack, a solution that creates a collection of task perspectives that can be carried across downstream tasks and used as a potential source of additional insights, as a backpack of skills that a robot can carry around and use when needed. We demonstrate the effectiveness and efficiency of our approach on four Ego4d benchmarks, outperforming current state-of-the-art methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!--section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Cite us</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
